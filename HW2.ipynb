{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YYdtMW7eZjk"
      },
      "source": [
        "#14757 Homework 2 (150 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONDj1bu0asVE"
      },
      "source": [
        "## **Due:** Wednesday October 2 at 3pm ET / 12 noon PT\n",
        "\n",
        "## Submission Instructions\n",
        "\n",
        "*   Download your completed notebook by clicking File->Download .ipynb and submit it on Gradescope\n",
        "*   Check your submission on Gradescope to make sure that all your code, code output and written responses appear correctly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4p_Fs74axfD"
      },
      "source": [
        "## Problem 1: Gradescope Autograder Placeholder (0 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhY8gXFca1uT"
      },
      "source": [
        "Gradescope requires that problem 1 be autograded for code submissions, but there are no autograded problems. Continue to problem 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgPiRgPROYhy"
      },
      "source": [
        "## Problem 2: Implementing Stochastic Gradient Descent For Support Vector Machines (70 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UKrNM2h36LT"
      },
      "source": [
        "In this problem you will train support vector machines (SVMs) using stochastic gradient descent (SGD) that you implement yourself. In other words, you are *not* allowed to use libraries other than `pandas`, `numpy`, `random` and `matplotlib.pyplot`.\n",
        "\n",
        "The data is from the Wisconsin Diagnostic Breast Cancer data set. There are 569 records. Each record has an id number, 10 continuous-valued features and a binary label (whether the cancer is malignant or benign.) The features are geometric and texture measurements extracted from images of breast cancer tumors.\n",
        "\n",
        "Run the cell below to perform the following steps:\n",
        "\n",
        "*   Load the data from the file `wdbc.data`\n",
        "*   Extract the labels and map them to 1 for malignant and -1 for benign\n",
        "*   Extract the 10 continuous-valued features\n",
        "*   Split the 569 records randomly into groups of 369 for training, 100 for validation and 100 for testing\n",
        "*   Standardize the features by subtracting the mean and dividing by the standard deviation of the training features\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA3FR__Z3613"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data'\n",
        "cancer_df = pd.read_table(url, sep=',', header=None)\n",
        "\n",
        "# Extract labels and map to 1 for malignant and -1 for benign\n",
        "labels = cancer_df[1].values\n",
        "labels = (labels == 'M').astype(int)\n",
        "labels = labels*2 - 1\n",
        "\n",
        "# Extract the 10 features by dropping the other ones\n",
        "cancer_df.drop(np.arange(0,32,3), axis = 1, inplace = True)\n",
        "cancer_df.drop(np.arange(1,32,3), axis = 1, inplace = True)\n",
        "X = cancer_df.to_numpy()\n",
        "\n",
        "def split_standardize(X, labels):\n",
        "\n",
        "  # Split records randomly into training, validation and test sets\n",
        "  N, M = X.shape\n",
        "  indices = [i for i in range(N)]\n",
        "  random.shuffle(indices)\n",
        "  X_train = X[indices[:369]]\n",
        "  X_val = X[indices[369:469]]\n",
        "  X_test = X[indices[469:]]\n",
        "  y_train = labels[indices[:369]]\n",
        "  y_val = labels[indices[369:469]]\n",
        "  y_test = labels[indices[469:]]\n",
        "\n",
        "  # Standardize features\n",
        "  data_mean = X_train.mean(axis=0)\n",
        "  data_std = X_train.std(axis=0)\n",
        "  X_train -= data_mean\n",
        "  X_train /= data_std\n",
        "  X_val -= data_mean\n",
        "  X_val /= data_std\n",
        "  X_test -= data_mean\n",
        "  X_test /= data_std\n",
        "\n",
        "  return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = split_standardize(X, labels)\n",
        "\n",
        "print('Shape of X_train:', X_train.shape)\n",
        "print('Shape of X_val:  ', X_val.shape)\n",
        "print('Shape of X_test: ', X_test.shape)\n",
        "print('Shape of y_train:', y_train.shape)\n",
        "print('Shape of y_val:  ', y_val.shape)\n",
        "print('Shape of y_test: ', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP68GAFVP7hR"
      },
      "source": [
        "**2.1** (50 pts) Complete the cell below to train a set of SVMs using SGD. Each SVM will have its own regularization constant from the list `[0.001, 0.01, 0.1, 1]`. (In the next part of this problem, you will choose the best regularization constant.)\n",
        "\n",
        "The SGD should use 20 epochs of 50 steps each. In each epoch, you should randomly hold out 50 training items for evaluation. Specifically you should evaluate the accuracy of the current classifier on these 50 items every 10 steps.\n",
        "\n",
        "The starter program includes code to plot the accuracy every 10 steps for each value of the regularization constant. You should tune the SGD step size so that each accuracy trajectory converges to near its final value quickly (within the first 50 steps) and then stays close to that value with only small deviations. To reliably achieve this behavior, the step size should become smaller as the epochs proceed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx7r6icqrPz0"
      },
      "source": [
        "class SVM:\n",
        "  def __init__(self, D, reg_const):\n",
        "    self.reg = reg_const\n",
        "    self.a = np.zeros(D)\n",
        "    self.b = 1\n",
        "\n",
        "# Constants\n",
        "num_epochs = 20\n",
        "steps_per_epoch = 50\n",
        "num_training_held_out = 50\n",
        "steps_per_acc_log = 10\n",
        "\n",
        "# Regularization constant values\n",
        "reg_values = [0.001, 0.01, 0.1, 1]\n",
        "# List of SVMs, one per regularization value\n",
        "svms = []\n",
        "# Sequences of accuracies to be plotted\n",
        "reg_accuracies = []\n",
        "\n",
        "\n",
        "\n",
        "# START EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "\n",
        "def compute_accuracy(X_set, y_set, svm):\n",
        "  return 0.5\n",
        "\n",
        "for reg_idx, reg_value in enumerate(reg_values):\n",
        "\n",
        "  # Instantiate SVM for this reg_value\n",
        "  N, D = X_train.shape\n",
        "  svm = SVM(D, reg_value)\n",
        "\n",
        "  # Sequence of accuracies for this reg_value\n",
        "  train_accuracies = []\n",
        "\n",
        "  for epoch_num in range(num_epochs):\n",
        "\n",
        "    for step_num in range(steps_per_epoch):\n",
        "\n",
        "      # Evaluate accuracy every 10 (i.e. steps_per_acc_log) steps\n",
        "      if (step_num % steps_per_acc_log) == 0:\n",
        "        acc = compute_accuracy(np.zeros((1, D)), np.zeros(D), svm)\n",
        "        train_accuracies.append(acc)\n",
        "\n",
        "  reg_accuracies.append(train_accuracies)\n",
        "  svms.append(svm)\n",
        "\n",
        "# END EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "\n",
        "\n",
        "\n",
        "# Plot accuracy every 10 (i.e. steps_per_acc_log) steps for all reg values\n",
        "steps = np.arange(0, num_epochs*steps_per_epoch, steps_per_acc_log)\n",
        "plt.rcParams.update({'font.size': 25})\n",
        "f, axarr = plt.subplots(len(reg_values), figsize = (50, 25))\n",
        "for reg_idx, reg_value in enumerate(reg_values):\n",
        "  axarr[reg_idx].plot(steps, reg_accuracies[reg_idx])\n",
        "  axarr[reg_idx].set_xticks(np.arange(0, num_epochs*steps_per_epoch, 50))\n",
        "  axarr[reg_idx].set_xlabel('step number')\n",
        "  axarr[reg_idx].set_ylabel('Accuracy, reg value=' + str(reg_value))\n",
        "  axarr[reg_idx].set_ylim(0, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM9Wlqjhm775"
      },
      "source": [
        "**2.2** (10 pts) What is your choice of the best regularization constant? Briefly (in 1-2 sentences) justify why you think this value is good. Support your justification with calculations in the cell below.\n",
        "\n",
        "WRITE YOUR ANSWER HERE:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsdCAtUZq4qT"
      },
      "source": [
        "# START EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "\n",
        "for reg_idx, reg_value in enumerate(reg_values):\n",
        "  print('reg_value =', reg_value)\n",
        "\n",
        "# END EDITING HERE - DON'T REMOVE THIS COMMENT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MO7MWqYz3VF"
      },
      "source": [
        "**2.3** (10 pts) Write code in the cell below to estimate the accuracy of the best classifier on the data held out so far."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQPIcc6ra2c6"
      },
      "source": [
        "# START EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "best_accuracy = 0.5\n",
        "# END EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "\n",
        "print('Estimated accuracy of best classifier:', best_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGHknXEild0S"
      },
      "source": [
        "## Problem 3: Attacking and Defending a Support Vector Machine (60 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uti4Ekfc6eH6"
      },
      "source": [
        "In this problem you will attack and defend an SVM classifier. Run the cell below to load, preprocess and display the data. The data set consists of the MNIST handwritten digits filtered to keep only the 0s and 1s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EsIpIiz-v-P"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def preprocess(features, labels):\n",
        "  # Keep only zeros and ones\n",
        "  idx = (labels == 0) | (labels == 1)\n",
        "  labels = labels[idx]\n",
        "  features = features[idx,:,:]\n",
        "\n",
        "  # Flatten each image from 28x28 to vector\n",
        "  features = features.reshape(features.shape[0], 28*28)\n",
        "\n",
        "  # Cast feature values from uint8 to float\n",
        "  features = features.astype(float)\n",
        "\n",
        "  print('Shape of labels:  ', labels.shape)\n",
        "  print('Shape of features:', features.shape)\n",
        "  print('Min pixel value:  ', features.min())\n",
        "  print('Max pixel value:  ', features.max())\n",
        "\n",
        "  return features, labels\n",
        "\n",
        "# Load and preprocess data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "print('TRAINING SET')\n",
        "X_train, y_train = preprocess(X_train, y_train)\n",
        "print('\\nTESTING SET')\n",
        "X_test, y_test = preprocess(X_test, y_test)\n",
        "\n",
        "# Print a row of training images and a row of testing images\n",
        "plt.rcParams.update({'font.size': 12})\n",
        "for i in range(8):\n",
        "  plt.subplot(2, 8, i+1)\n",
        "  plt.imshow(X_train[i].reshape(28, 28), cmap=plt.get_cmap('gray'))\n",
        "  plt.subplot(2, 8, i+9)\n",
        "  plt.imshow(X_test[i].reshape(28, 28), cmap=plt.get_cmap('gray'))\n",
        "plt.subplots_adjust(bottom=0, top=1, left=0, right=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8L4WZ1uDokD"
      },
      "source": [
        "Run the cell below to train and evaluate the SVM classifier. The SVM kernel is linear in that each pixel $x_i$ is associated directly with a weight $a_i$ in the expression $\\mathbf{a}^T\\mathbf{x}+b$. The output shows the accuracy on the testing set as well as the two testing images that are misclassified.\n",
        "\n",
        "Helpful documentation: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpGZZTVF3IbW"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def display_misclassified_images(features, labels, preds):\n",
        "  idx = (labels != preds)\n",
        "  misclassified = features[idx]\n",
        "  num_misclassified = misclassified.shape[0]\n",
        "  print('Indices of misclassified test images:', np.nonzero(idx))\n",
        "  for i in range(num_misclassified):\n",
        "    plt.subplot(1, num_misclassified, i+1)\n",
        "    plt.imshow(misclassified[i].reshape(28, 28), cmap=plt.get_cmap('gray'))\n",
        "\n",
        "# Initialize, train and evaluate SVM\n",
        "classifier = SVC(kernel='linear', random_state=0)\n",
        "classifier.fit(X_train, y_train)\n",
        "predictions = classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print('Accuracy:', \"{:.3f}\".format(accuracy))\n",
        "display_misclassified_images(X_test, y_test, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WX3LbEHza_t8"
      },
      "source": [
        "### Single pixel attack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cREX2x7vRezV"
      },
      "source": [
        "**3.1** (20 pts) You will perform an open box evasion attack by modifying a single pixel in the entire testing set. Specifically, you should provide values or expressions for:\n",
        "\n",
        "*   `img_idx` to choose an image from the testing set\n",
        "*   `pix_idx` to choose a pixel\n",
        "*   `pix_val` to choose the pixel's new value, between 0 and 255 inclusive.\n",
        "\n",
        "For credit the image you modify must evade the classifier, but cannot be one of the two already misclassified images shown above. In other words, the cell below should display a total of three misclassified test images.\n",
        "\n",
        "Hint: Start by setting `pix_idx` to be the most important pixel in the `classifier` model. For your convenience we have extracted the SVM model parameters into the variables `a` and `b`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnpD0_K3vnax"
      },
      "source": [
        "# SVM model parameters a and b\n",
        "a = classifier.coef_\n",
        "b = classifier.intercept_\n",
        "\n",
        "# Copy X_test to X_test_1pix for this single pixel attack\n",
        "X_test_1pix = X_test.copy()\n",
        "\n",
        "# START EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "img_idx = 0       # not equal to 1664 or 2031\n",
        "pix_idx = 0       # 0 <= pix_idx < 28*28\n",
        "pix_val = 0       # 0 <= pix_val <= 255\n",
        "# END EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "\n",
        "print('img_idx:', img_idx)\n",
        "print('pix_idx:', pix_idx)\n",
        "print('pix_val:', pix_val)\n",
        "\n",
        "# Modify the single pixel\n",
        "X_test_1pix[img_idx, pix_idx] = pix_val\n",
        "\n",
        "# Display misclassified test images\n",
        "predictions_1pix = classifier.predict(X_test_1pix)\n",
        "display_misclassified_images(X_test_1pix, y_test, predictions_1pix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J46hpqWh2ta4"
      },
      "source": [
        "### Additive noise attack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBt1Hv6R2m1V"
      },
      "source": [
        "**3.2** (20 pts) Perform another open box evasion reliability attack that significantly reduces the accuracy on the modified testing set. For full credit you must bring the accuracy below 65%. This time you are allowed to modify all the pixels in the testing set `X_test_noisy` by adding or subtracting noise subject to the following constraints:\n",
        "\n",
        "*   You can increase or decrease each original pixel value by at most `max_noise = 32`.\n",
        "*   All modified pixel values must fall within 0 and 255 inclusive.\n",
        "\n",
        "For example, if a certain pixel value is originally 20, then the modified value can be between 0 and 52 inclusive. As in the previous part you may use the SVM model parameters `a` and `b` since this is an open box attack. You may also use the testing labels `y_test` but not modify them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aL-r5CFs7wTx"
      },
      "source": [
        "# Copy X_test to X_test_noisy for this additive noise attack\n",
        "X_test_noisy = X_test.copy()\n",
        "\n",
        "# Maximum offset between original and modified pixel values\n",
        "max_noise = 32\n",
        "\n",
        "# START EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "\n",
        "\n",
        "# END EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "\n",
        "# Find accuracy on modified testing set\n",
        "predictions_noisy = classifier.predict(X_test_noisy)\n",
        "accuracy_noisy = accuracy_score(y_test, predictions_noisy)\n",
        "print('Accuracy:', \"{:.3f}\".format(accuracy_noisy))\n",
        "\n",
        "# Display the first 8 modified test images\n",
        "for i in range(8):\n",
        "  plt.subplot(2, 8, i+1)\n",
        "  plt.imshow(X_test_noisy[i].reshape(28, 28), cmap=plt.get_cmap('gray'))\n",
        "plt.subplots_adjust(bottom=0, top=1, left=0, right=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FgHFZqy77oX"
      },
      "source": [
        "### Defending against the additive noise attack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5adNZS88BAo"
      },
      "source": [
        "**3.3** (20 pts) You will now train a `robust_classifier` that performs well on both the original testing set `X_test` and the modified testing set `X_test_noisy`. Edit the training set (`X_train_robust`, `y_train_robust`), so that the accuracy on the concatenated testing set (`X_test_all`, `y_test_all`) exceeds 99%.\n",
        "\n",
        "Note that you *cannot* use any information from the testing set in the training of `robust_classifier`, but you can use information from the training set and from the model `classifier`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4jgjIVNq0HH"
      },
      "source": [
        "# Concatenate X_test and X_test_noisy into a single testing set X_test_all\n",
        "X_test_all = np.concatenate((X_test, X_test_noisy), axis=0)\n",
        "# Form corresponding y_test_all\n",
        "y_test_all = np.concatenate((y_test, y_test), axis=0)\n",
        "\n",
        "# START EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "X_train_robust = X_train.copy()\n",
        "y_train_robust = y_train.copy()\n",
        "# END EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "\n",
        "# Train robust classifier\n",
        "robust_classifier = SVC(kernel='linear', random_state=0)\n",
        "robust_classifier.fit(X_train_robust, y_train_robust)\n",
        "\n",
        "# Evaluate robust classifier on combined test set\n",
        "predictions_robust = robust_classifier.predict(X_test_all)\n",
        "accuracy_robust = accuracy_score(y_test_all, predictions_robust)\n",
        "print('Accuracy:', \"{:.3f}\".format(accuracy_robust))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 4: Classification using Random Forests (20 pts)"
      ],
      "metadata": {
        "id": "RuspyO_MSxSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this problem you will use scikit-learn's random forest classifier to distinguish between poisonous and edible mushrooms. Run the cell below to load and inspect the data. Notice that the attributes are categorical. You can look up their meaning at http://archive.ics.uci.edu/ml/datasets/Mushroom."
      ],
      "metadata": {
        "id": "9HUoLtDxTagq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data'\n",
        "df = pd.read_table(url, sep=',', header=None)\n",
        "display(df)"
      ],
      "metadata": {
        "id": "XUB4uMy3TgFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.1** (10 pts) Build a random forest to classify a mushroom as poisonous or edible based on its other attributes. Print out the class confusion matrix.\n",
        "\n",
        "Note that a random forest classifier cannot handle categorical features. Convert each attribute into a set of indicator (or dummy) variables using `pd.get_dummies()`. This process is also known as one-hot encoding."
      ],
      "metadata": {
        "id": "WfFYw79ATlhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# START EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "\n",
        "\n",
        "# END EDITING HERE - DON'T REMOVE THIS COMMENT"
      ],
      "metadata": {
        "id": "3eQU1Q1qTpt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.2** (5 pts) What is the probability of being poisoned by eating a mushroom that your classifier predicts is edible?\n",
        "\n",
        "WRITE YOUR ANSWER HERE:"
      ],
      "metadata": {
        "id": "KwysCzsPTy4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.3** (5 pts) One-hot encoding converts each categorical attribute into several binary attributes. Explain why this process is better than keeping the total number of attributes the same by mapping each attribute's possible values to consecutive integers (e.g. for attribute 1, map b=0, c=1, x=2, f=3, k=4, s=5.) Limit your answer to 1-3 sentences.\n",
        "\n",
        "WRITE YOUR ANSWER HERE:"
      ],
      "metadata": {
        "id": "3NRgULXNT6Yy"
      }
    }
  ]
}