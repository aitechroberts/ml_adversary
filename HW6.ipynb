{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YYdtMW7eZjk"
      },
      "source": [
        "#14757 Homework 6 (150 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AP4ATuX2krz"
      },
      "source": [
        "## **Due:** Wednesday December 4 at 3pm ET / 12 noon PT\n",
        "\n",
        "## Submission Instructions\n",
        "\n",
        "*   Download your completed notebook by clicking File->Download .ipynb and submit it on Gradescope\n",
        "*   Check your submission on Gradescope to make sure that all your code, code output and written responses appear correctly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGm30ifMxWJ4"
      },
      "source": [
        "## Problem 1: Gradescope Autograder Placeholder (0 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHRNNNe1xXDA"
      },
      "source": [
        "Gradescope requires that problem 1 be autograded for code submissions, but there are no autograded problems. Continue to problem 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgPiRgPROYhy"
      },
      "source": [
        "## Problem 2: Image Segmentation Using Expectation-Maximization (75 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hb5j84JmA0DP"
      },
      "source": [
        "In this problem you will implement and compare a few  ways of segmenting an image based on color.\n",
        "\n",
        "Run the cell below to load an [image of a colorful flower](http://www.andrew.cmu.edu/user/dvaroday/14757/data/hw6/strelitzia.jpg), and also run $k$-means clustering on the pixels with 3 different values for number of clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzYEwisgYw0Z"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from urllib.request import urlopen\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "# Load image\n",
        "url = 'http://www.andrew.cmu.edu/user/dvaroday/14757/data/hw6/strelitzia.jpg'\n",
        "img = np.array(plt.imread(urlopen(url), 0), dtype=float)/255\n",
        "\n",
        "# Reshape image data into 2-D array\n",
        "w, h, d = img.shape\n",
        "img_array = img.reshape(w*h, d)\n",
        "\n",
        "# Run k-means on the pixels with 3 different values for number of clusters\n",
        "num_clusters = [2, 5, 10]\n",
        "kmeans_list = []\n",
        "for i in range(len(num_clusters)):\n",
        "  kmeans = KMeans(n_clusters=num_clusters[i], n_init='auto')\n",
        "  kmeans.fit_predict(img_array)\n",
        "  kmeans_list.append(kmeans)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQTiGOLRFWsz"
      },
      "source": [
        "**2.1** (20 pts) Complete the function `reconstruct_image()` in the cell below so that it returns the image with each pixel replaced by its cluster center. The cell will display the 3 segmentations from the $k$-means clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5lKgRCGFWTu"
      },
      "outputs": [],
      "source": [
        "# Reconstruct the image by replacing each pixel by its cluster center\n",
        "def reconstruct_image(centers, labels, w, h, d):\n",
        "    image = np.zeros((w, h, d))\n",
        "\n",
        "    # START EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "\n",
        "    # END EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "\n",
        "    return image\n",
        "\n",
        "# Display reconstructed images\n",
        "fig = plt.figure(figsize=(12, 12))\n",
        "axarr = fig.subplots(1, len(num_clusters))\n",
        "for i in range(len(num_clusters)):\n",
        "  axarr[i].imshow(reconstruct_image(kmeans_list[i].cluster_centers_,\n",
        "                                    kmeans_list[i].labels_, w, h, d))\n",
        "  axarr[i].set_title('k-means, ' + str(num_clusters[i]) + ' clusters')\n",
        "  axarr[i].axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctf94KOwwbIy"
      },
      "source": [
        "**2.2** (40 pts) In the `EM` class below complete the methods `__init__()`, `e_step()`, `m_step()` and `set_labels()` to implement color segmentation using the EM algorithm. In particular you should edit the constructor `__init__()` so that it initializes the algorithm with the clustering provided by the `kmeans` argument. Use `multivariate_normal.pdf()` to implement the calculation in `e_step()` and as a consequence make it numerically stable: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html\n",
        "\n",
        "The segmentation images you obtain in this part of the problem should be very similar to the ones obtained from $k$-means only. But if you look carefully you should be able to find differences in the colors and pixel assignments of the clusters, especially in the case of 10 clusters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vh9PJuWN3SnI"
      },
      "outputs": [],
      "source": [
        "# START EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "class EM:\n",
        "\n",
        "  # Constructs EM object\n",
        "  def __init__(self, img_array, kmeans, cov, num_iters=10):\n",
        "    self.img_array = img_array\n",
        "    self.cov = cov\n",
        "    self.num_iters = num_iters\n",
        "\n",
        "    # Initializing the EM object to the clustering passed in as kmeans\n",
        "    self.num_clusters = kmeans.cluster_centers_.shape[0]\n",
        "    self.means = np.zeros((self.num_clusters, 3))\n",
        "    self.labels = np.zeros(img_array.shape[0], dtype=int)\n",
        "    self.mix_weights = np.ones(self.num_clusters)\n",
        "    self.weights = np.zeros((self.num_clusters, img_array.shape[0]))\n",
        "\n",
        "  # Updates weight matrix\n",
        "  # Hint: use multivariate_normal.pdf() to implement part of the calculation\n",
        "  def e_step(self):\n",
        "    pass\n",
        "\n",
        "  # Updates means and mixing weights\n",
        "  def m_step(self):\n",
        "    pass\n",
        "\n",
        "  # Sets pixel labels after EM iterations are complete\n",
        "  def set_labels(self):\n",
        "    pass\n",
        "\n",
        "  # Performs image color segmentation\n",
        "  def segment(self):\n",
        "    for iter in range(self.num_iters):\n",
        "      em.e_step()\n",
        "      em.m_step()\n",
        "    em.set_labels()\n",
        "# END EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "\n",
        "\n",
        "# Display reconstructed images\n",
        "fig = plt.figure(figsize=(12, 12))\n",
        "axarr = fig.subplots(1, len(num_clusters))\n",
        "for i in range(len(num_clusters)):\n",
        "  em = EM(img_array, kmeans=kmeans_list[i], cov=0.01*np.identity(3))\n",
        "  em.segment()\n",
        "  axarr[i].imshow(reconstruct_image(em.means, em.labels, w, h, d))\n",
        "  axarr[i].set_title('EM with identity covariance, '\n",
        "                     + str(num_clusters[i]) + ' clusters')\n",
        "  axarr[i].axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOC0xU47MptG"
      },
      "source": [
        "**2.3** (5 pts) You obtained the segmentation images in the cell above by using the covariance matrix $\\frac{1}{100}I$ for all clusters in the EM model. In the cell below replace $\\frac{1}{100}I$ with the covariance of all the pixels in the original image. The resulting segmentation images should look quite different from the ones you have seen before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tm-uT7anM6U1"
      },
      "outputs": [],
      "source": [
        "# Display reconstructed images\n",
        "fig = plt.figure(figsize=(12, 12))\n",
        "axarr = fig.subplots(1, len(num_clusters))\n",
        "for i in range(len(num_clusters)):\n",
        "\n",
        "  # EDIT THE LINE BELOW - DON'T REMOVE THIS COMMENT\n",
        "  em = EM(img_array, kmeans=kmeans_list[i], cov=0.01*np.identity(3))\n",
        "\n",
        "  em.segment()\n",
        "  axarr[i].imshow(reconstruct_image(em.means, em.labels, w, h, d))\n",
        "  axarr[i].set_title('EM with image covariance, '\n",
        "                     + str(num_clusters[i]) + ' clusters')\n",
        "  axarr[i].axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSIQ3XgBO5zd"
      },
      "source": [
        "**2.4** (10 pts) Notice that setting the `cov` argument to the covariance of the original image results in cleaner background segmentations than using $\\frac{1}{100}I$. In other words more of the background pixels end up in one background cluster. As a result the foreground clusters become smaller and more specific, and so are represented by more accurate colors. Explain in a paragraph why changing the covariance matrix in this way makes more of the background pixels end up in one background cluster.\n",
        "\n",
        "WRITE YOUR ANSWER HERE:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWVJAS3HWW4K"
      },
      "source": [
        "## Problem 3: Search Engine Optimization (75 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHYisYs5Xy3k"
      },
      "source": [
        "In this problem you will explore a web graph that was scraped from the web some time during the early years of Google. You will consider how Yahoo might have attacked Google's PageRank algorithm to elevate the Yahoo homepage in Google search results and how Google may have reacted.\n",
        "\n",
        "The nodes of the web graph are URLs associated with the search term `California`. The edges denote links from one URL to another.\n",
        "\n",
        "Run the cell below to load the URLs and links into data frames called `nodes` and `edges`, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqjeXkNaWgJy"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-network\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.sparse import csr_matrix\n",
        "from sknetwork.ranking import PageRank\n",
        "from warnings import filterwarnings\n",
        "\n",
        "# Load node (URL) and edge (link) data\n",
        "url = 'https://www.cs.cornell.edu/courses/cs685/2002fa/data/gr0.California'\n",
        "\n",
        "num_nodes = 9664\n",
        "edges = pd.read_table(url, sep='\\s', header=None, skiprows=num_nodes,\n",
        "                   usecols=[1, 2], names=['from', 'to'], engine='python')\n",
        "num_edges = len(edges)\n",
        "nodes = pd.read_table(url, sep='\\s', header=None, skipfooter=num_edges,\n",
        "                   usecols=[1, 2], names=['idx', 'url'], engine='python')\n",
        "\n",
        "display(nodes)\n",
        "display(edges)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIf-Ky_TeVeR"
      },
      "source": [
        "Lets estimate the year in which this data set was scraped. Run the cell below to plot the number of occurences of the strings \"1994\" through \"2004\" in the data set's URLs.\n",
        "\n",
        "*Yahoo was founded in 1994 and incorporated in 1995. Google began as a research project in 1996, it registered its domain in 1997 and incorporated in 1998. Also in 1998 Yahoo turned down an opportunity to buy Google for \\$1 million. In 2002 Yahoo made a \\$3 billion offer to buy Google but this time Google turned down the offer. Yahoo Search was actually powered by Google for a few years until 2004.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kILdpya_eeTJ"
      },
      "outputs": [],
      "source": [
        "years = range(1994,2005)\n",
        "counts = []\n",
        "for year in years:\n",
        "  counts.append(nodes['url'].str.contains(str(year)).sum())\n",
        "\n",
        "plt.figure(figsize = (4,4))\n",
        "plt.bar(years, counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRY4jaH4eXzB"
      },
      "source": [
        "**3.1** (5 pts) In which year was this data set scraped from the web? If you are not sure, take a look at the URLs that contain the year strings.\n",
        "\n",
        "WRITE YOUR ANSWER HERE:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjJeGt6JEra4"
      },
      "source": [
        "In the cell below, the edge list is converted into an adjacency matrix. The $(i, j)$ entry of `adjacency` is $1$ if there is a link from the $i$th URL to the $j$th URL; and $0$ otherwise. We use a sparse matrix representation for `adjacency` because it is mostly zeroes.\n",
        "\n",
        "**3.2** (10 pts) Complete the function `search_rank()` in the cell below. The function should return the positive integer rank of `target_url` when the PageRank algorithm is applied to `adj_matrix`. In other words, `search_rank(yahoo_home_url, adjacency)` should return position of the Yahoo homepage in the list of results for the search on `California`; for example, it would return $1$ if the Yahoo homepage were the top ranking URL.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56IsybwSH0u3"
      },
      "outputs": [],
      "source": [
        "# Suppress warnings about changing the sparsity structure of csr_matrix\n",
        "filterwarnings(\"ignore\", message=\"Changing the jsparsity structure.*\")\n",
        "\n",
        "# Convert edge list to adjacency matrix (sparse representation)\n",
        "adjacency = csr_matrix((np.ones(len(edges)), (edges['from'], edges['to'])),\n",
        "                       shape=(num_nodes, num_nodes), dtype=int)\n",
        "\n",
        "def search_rank(target_url, adj_matrix):\n",
        "  pagerank = PageRank()\n",
        "  stationary_distribution = pagerank.fit_predict(adj_matrix)\n",
        "\n",
        "  # START EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "  rank = 0\n",
        "  # END EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "\n",
        "  return rank\n",
        "\n",
        "yahoo_home_url = \"http://www.yahoo.com/\"\n",
        "print('Yahoo homepage search rank:', search_rank(yahoo_home_url, adjacency))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1RYNlyPHkBq"
      },
      "source": [
        "**3.3** (10 pts) Suppose Yahoo attempts to improve the search ranking of its homepage by increasing its number of backlinks. Specifically Yahoo adds links that point to its homepage from all the **other** pages it controls. There are in total 222 pages controlled by Yahoo (besides the homepage itself) including ones in the domains `yahoo.com`, `yahoo.co.uk`, `yahoo.ie`, etc. Edit `adj_backlinks` in the cell below to simulate this attack and print out the new search rank for the Yahoo homepage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIUN5W9PVTto"
      },
      "outputs": [],
      "source": [
        "adj_backlinks = adjacency.copy()\n",
        "\n",
        "# START EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "\n",
        "# END EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "\n",
        "print('Yahoo homepage search rank with more backlinks:',\n",
        "      search_rank(yahoo_home_url, adj_backlinks))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58AIcXEDy6BO"
      },
      "source": [
        "**3.4** (10 pts) The addition of the backlinks went undetected by Google, so Yahoo decides to improve its homepage's search ranking even further using a link sculpting approach. Yahoo engineers figure out a way to [obfuscate links](https://www.drupal.org/project/link_obfuscator) so that they work normally for human users but become invisible to robots. Edit `adj_sculpting` in the cell below to improve the Yahoo homepage's search rank as much as possible by applying link sculpting to all 222 pages that Yahoo controls (besides the homepage itself.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f642iOQyEio-"
      },
      "outputs": [],
      "source": [
        "adj_sculpting = adj_backlinks.copy()\n",
        "\n",
        "# START EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "\n",
        "# END EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "\n",
        "print('Yahoo homepage search rank with link sculpting:',\n",
        "      search_rank(yahoo_home_url, adj_sculpting))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBvREHt-Eo8H"
      },
      "source": [
        "**3.5** (10 pts) Now Google discovers and reverse engineers the link obfuscation. Not only does Google set its robot to follow all the human-clickable links, but it imposes an additional penalty on Yahoo: any links from Yahoo webpages to the Yahoo homepage will be ignored in the PageRank algorithm. Edit `adj_penalty` in the cell below to print out the Yahoo homepage's search ranking after the changes to Google's algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_RoSfpuJ-Ej"
      },
      "outputs": [],
      "source": [
        "adj_penalty = adj_sculpting.copy()\n",
        "\n",
        "# START EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "\n",
        "# END EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "\n",
        "print('Yahoo homepage search rank after Google penalty:',\n",
        "      search_rank(yahoo_home_url, adj_penalty))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QL-hmhG2KAsS"
      },
      "source": [
        "**3.6** (10 pts) Why is the Yahoo homepage's search ranking worse after Google's penalty than in the beginning? Use 1-2 sentences in your answer.\n",
        "\n",
        "WRITE YOUR ANSWER HERE:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2421ZfFiLGgy"
      },
      "source": [
        "**3.7** (5 pts) After dropping in the search ranking, some Yahoo executives decide to make one more effort. They purchase a link to the Yahoo homepage from the homepage of the promisingly-named startup LinkExchange. Edit `adj_buy_link` in the cell below to print out the Yahoo homepage's search ranking after a link to it is added on the LinkExchange homepage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOJaiV5gVvye"
      },
      "outputs": [],
      "source": [
        "link_exch_home_url = \"http://www.linkexchange.com/\"\n",
        "adj_buy_link = adj_penalty.copy()\n",
        "\n",
        "# START EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "\n",
        "# END EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "\n",
        "print('Yahoo homepage search rank after buying link:',\n",
        "      search_rank(yahoo_home_url, adj_buy_link))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFmSH085V04V"
      },
      "source": [
        "**3.8** (5 pts) A Yahoo engineer suggests that it would be helpful for the Yahoo homepage to link back to the LinkExchange homepage. Edit `adj_link_back` in the cell below to print out the Yahoo homepage's search ranking after it links back to the LinkExchange homepage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCc8AtE0W7JH"
      },
      "outputs": [],
      "source": [
        "adj_link_back = adj_buy_link.copy()\n",
        "\n",
        "# START EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "\n",
        "# END EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "\n",
        "print('Yahoo homepage search rank after linking back:',\n",
        "      search_rank(yahoo_home_url, adj_link_back))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXXIrYbzXGFe"
      },
      "source": [
        "**3.9** (10 pts) Why does linking back to the LinkExchange homepage improve the Yahoo homepage's search ranking? Use 1-2 sentences in your answer.\n",
        "\n",
        "WRITE YOUR ANSWER HERE:"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}